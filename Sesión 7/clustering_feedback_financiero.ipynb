{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnÃ¡lisis de Clustering No Supervisado: SegmentaciÃ³n de Feedback Financiero\n",
    "\n",
    "## Objetivo del Ejercicio\n",
    "\n",
    "Aplicar tÃ©cnicas de **Machine Learning No Supervisado** para identificar patrones y segmentos naturales en el feedback de clientes de productos financieros. Este anÃ¡lisis permite:\n",
    "\n",
    "- Identificar grupos de clientes con comportamientos y experiencias similares\n",
    "- Descubrir insights ocultos en datos no etiquetados\n",
    "- Priorizar acciones de mejora basadas en segmentos crÃ­ticos\n",
    "- Generar hipÃ³tesis para anÃ¡lisis supervisados posteriores\n",
    "\n",
    "## Estructura del Notebook\n",
    "\n",
    "1. **ConfiguraciÃ³n del Entorno**\n",
    "2. **ExploraciÃ³n y PreparaciÃ³n de Datos**\n",
    "3. **IngenierÃ­a de CaracterÃ­sticas**\n",
    "4. **AnÃ¡lisis de Clustering K-Means**\n",
    "5. **Clustering JerÃ¡rquico**\n",
    "6. **DBSCAN - Density-Based Clustering**\n",
    "7. **ComparaciÃ³n de Modelos**\n",
    "8. **InterpretaciÃ³n de Negocio**\n",
    "9. **Recomendaciones EstratÃ©gicas**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE 1: ConfiguraciÃ³n del Entorno y LibrerÃ­as\n",
    "\n",
    "InstalaciÃ³n y carga de todas las dependencias necesarias para el anÃ¡lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# ConfiguraciÃ³n de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ“ LibrerÃ­as bÃ¡sicas cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LibrerÃ­as de Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# LibrerÃ­as de procesamiento de texto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "print(\"âœ“ LibrerÃ­as de ML y NLP cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 2: Carga y ExploraciÃ³n Inicial de Datos\n",
    "\n",
    "AnÃ¡lisis exploratorio para comprender la estructura y calidad del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "df = pd.read_csv('financial_feedback_data.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "print(f\"\\nPrimeras 5 observaciones:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INFORMACIÃ“N DE TIPOS DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de valores faltantes\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÃLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Valores_Faltantes': df.isnull().sum(),\n",
    "    'Porcentaje': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Valores_Faltantes'] > 0].sort_values('Valores_Faltantes', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    display(missing_data)\n",
    "else:\n",
    "    print(\"âœ“ No se detectaron valores faltantes en el dataset\")\n",
    "\n",
    "# EstadÃ­sticas descriptivas de variables numÃ©ricas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTADÃSTICAS DESCRIPTIVAS - VARIABLES NUMÃ‰RICAS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de variables categÃ³ricas\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUCIÃ“N DE VARIABLES CATEGÃ“RICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_cols = ['Producto', 'Canal_Feedback', 'Cliente_Tipo', 'RegiÃ³n']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(\"-\" * 40)\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentage = (value_counts / len(df) * 100).round(2)\n",
    "    result = pd.DataFrame({\n",
    "        'Frecuencia': value_counts,\n",
    "        'Porcentaje': percentage\n",
    "    })\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 3: AnÃ¡lisis Exploratorio Visual\n",
    "\n",
    "Visualizaciones para identificar patrones preliminares en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistribuciÃ³n de Rating\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('AnÃ¡lisis de DistribuciÃ³n de Variables Clave', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rating distribution\n",
    "sns.countplot(data=df, x='Rating', ax=axes[0, 0], palette='viridis')\n",
    "axes[0, 0].set_title('DistribuciÃ³n de Rating', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Rating')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "# AÃ±adir valores en las barras\n",
    "for container in axes[0, 0].containers:\n",
    "    axes[0, 0].bar_label(container)\n",
    "\n",
    "# Producto distribution\n",
    "product_counts = df['Producto'].value_counts()\n",
    "axes[0, 1].barh(product_counts.index, product_counts.values, color='coral')\n",
    "axes[0, 1].set_title('DistribuciÃ³n por Producto', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Frecuencia')\n",
    "axes[0, 1].set_ylabel('Producto')\n",
    "\n",
    "# Canal Feedback distribution\n",
    "canal_counts = df['Canal_Feedback'].value_counts()\n",
    "axes[1, 0].barh(canal_counts.index, canal_counts.values, color='lightblue')\n",
    "axes[1, 0].set_title('DistribuciÃ³n por Canal de Feedback', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Frecuencia')\n",
    "axes[1, 0].set_ylabel('Canal')\n",
    "\n",
    "# RegiÃ³n distribution\n",
    "sns.countplot(data=df, x='RegiÃ³n', ax=axes[1, 1], palette='Set2')\n",
    "axes[1, 1].set_title('DistribuciÃ³n por RegiÃ³n', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('RegiÃ³n')\n",
    "axes[1, 1].set_ylabel('Frecuencia')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for container in axes[1, 1].containers:\n",
    "    axes[1, 1].bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de Rating por Producto y Canal\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle('Rating Promedio por CategorÃ­as', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rating por Producto\n",
    "rating_por_producto = df.groupby('Producto')['Rating'].mean().sort_values(ascending=True)\n",
    "axes[0].barh(rating_por_producto.index, rating_por_producto.values, color='steelblue')\n",
    "axes[0].set_title('Rating Promedio por Producto', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Rating Promedio')\n",
    "axes[0].set_xlim(0, 5)\n",
    "axes[0].axvline(x=df['Rating'].mean(), color='red', linestyle='--', label=f'Media Global: {df[\"Rating\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Rating por Canal\n",
    "rating_por_canal = df.groupby('Canal_Feedback')['Rating'].mean().sort_values(ascending=True)\n",
    "axes[1].barh(rating_por_canal.index, rating_por_canal.values, color='darkorange')\n",
    "axes[1].set_title('Rating Promedio por Canal de Feedback', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Rating Promedio')\n",
    "axes[1].set_xlim(0, 5)\n",
    "axes[1].axvline(x=df['Rating'].mean(), color='red', linestyle='--', label=f'Media Global: {df[\"Rating\"].mean():.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 4: IngenierÃ­a de CaracterÃ­sticas\n",
    "\n",
    "TransformaciÃ³n de variables categÃ³ricas y creaciÃ³n de features numÃ©ricas para clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataframe para no modificar el original\n",
    "df_clustering = df.copy()\n",
    "\n",
    "# Convertir fecha a datetime y extraer caracterÃ­sticas temporales\n",
    "df_clustering['Fecha'] = pd.to_datetime(df_clustering['Fecha'])\n",
    "df_clustering['Mes'] = df_clustering['Fecha'].dt.month\n",
    "df_clustering['Trimestre'] = df_clustering['Fecha'].dt.quarter\n",
    "df_clustering['Dia_Semana'] = df_clustering['Fecha'].dt.dayofweek\n",
    "\n",
    "# AnÃ¡lisis de longitud de comentarios\n",
    "df_clustering['Longitud_Comentario'] = df_clustering['Comentario'].str.len()\n",
    "df_clustering['Num_Palabras'] = df_clustering['Comentario'].str.split().str.len()\n",
    "\n",
    "# ClasificaciÃ³n manual de sentimiento basado en Rating\n",
    "def clasificar_sentimiento(rating):\n",
    "    if rating <= 2:\n",
    "        return 'Negativo'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positivo'\n",
    "\n",
    "df_clustering['Sentimiento_Derivado'] = df_clustering['Rating'].apply(clasificar_sentimiento)\n",
    "\n",
    "print(\"âœ“ CaracterÃ­sticas temporales y de texto creadas\")\n",
    "print(f\"\\nNuevas caracterÃ­sticas:\")\n",
    "print(f\"  - Mes, Trimestre, Dia_Semana\")\n",
    "print(f\"  - Longitud_Comentario, Num_Palabras\")\n",
    "print(f\"  - Sentimiento_Derivado\")\n",
    "\n",
    "display(df_clustering[['ID_Comentario', 'Rating', 'Mes', 'Trimestre', \n",
    "                        'Longitud_Comentario', 'Num_Palabras', 'Sentimiento_Derivado']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding de variables categÃ³ricas\n",
    "print(\"=\" * 80)\n",
    "print(\"ENCODING DE VARIABLES CATEGÃ“RICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Label Encoding para variables con cardinalidad baja\n",
    "le_dict = {}\n",
    "categorical_features = ['Producto', 'Canal_Feedback', 'Cliente_Tipo', 'RegiÃ³n', 'Sentimiento_Derivado']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_clustering[f'{feature}_Encoded'] = le.fit_transform(df_clustering[feature])\n",
    "    le_dict[feature] = le\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Clases: {list(le.classes_)}\")\n",
    "    print(f\"  Encodings: {list(range(len(le.classes_)))}\")\n",
    "\n",
    "print(\"\\nâœ“ Variables categÃ³ricas codificadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelecciÃ³n de features para clustering\n",
    "features_for_clustering = [\n",
    "    'Rating',\n",
    "    'Producto_Encoded',\n",
    "    'Canal_Feedback_Encoded',\n",
    "    'Cliente_Tipo_Encoded',\n",
    "    'RegiÃ³n_Encoded',\n",
    "    'Mes',\n",
    "    'Trimestre',\n",
    "    'Longitud_Comentario',\n",
    "    'Num_Palabras',\n",
    "    'Sentimiento_Derivado_Encoded'\n",
    "]\n",
    "\n",
    "X = df_clustering[features_for_clustering].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MATRIZ DE CARACTERÃSTICAS PARA CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDimensiones: {X.shape}\")\n",
    "print(f\"\\nFeatures seleccionadas:\")\n",
    "for i, feat in enumerate(features_for_clustering, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\nEstadÃ­sticas de la matriz de features:\")\n",
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstandarizaciÃ³n de features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_clustering)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTANDARIZACIÃ“N DE CARACTERÃSTICAS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nâœ“ Features estandarizadas con StandardScaler (media=0, std=1)\")\n",
    "print(f\"\\nDimensiones post-escalamiento: {X_scaled.shape}\")\n",
    "print(\"\\nPrimeras 5 observaciones estandarizadas:\")\n",
    "display(X_scaled_df.head())\n",
    "\n",
    "print(\"\\nValidaciÃ³n de estandarizaciÃ³n:\")\n",
    "print(f\"  Media de features: {X_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"  Desv. Est. de features: {X_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 5: ReducciÃ³n Dimensional con PCA\n",
    "\n",
    "AplicaciÃ³n de PCA para visualizaciÃ³n y anÃ¡lisis de varianza explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA para visualizaciÃ³n\n",
    "pca = PCA(n_components=min(10, X_scaled.shape[1]))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÃLISIS DE COMPONENTES PRINCIPALES (PCA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Varianza explicada\n",
    "variance_df = pd.DataFrame({\n",
    "    'Componente': [f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))],\n",
    "    'Varianza_Explicada': pca.explained_variance_ratio_,\n",
    "    'Varianza_Acumulada': np.cumsum(pca.explained_variance_ratio_)\n",
    "})\n",
    "\n",
    "print(\"\\nVarianza explicada por componente:\")\n",
    "display(variance_df)\n",
    "\n",
    "# VisualizaciÃ³n de varianza explicada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "            pca.explained_variance_ratio_, \n",
    "            color='steelblue', alpha=0.7)\n",
    "axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             pca.explained_variance_ratio_, \n",
    "             marker='o', color='red', linewidth=2)\n",
    "axes[0].set_xlabel('Componente Principal', fontsize=12)\n",
    "axes[0].set_ylabel('Varianza Explicada', fontsize=12)\n",
    "axes[0].set_title('Scree Plot - Varianza Explicada por Componente', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Varianza acumulada\n",
    "axes[1].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             np.cumsum(pca.explained_variance_ratio_), \n",
    "             marker='o', linewidth=2, color='darkgreen')\n",
    "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% Varianza')\n",
    "axes[1].axhline(y=0.9, color='orange', linestyle='--', label='90% Varianza')\n",
    "axes[1].set_xlabel('NÃºmero de Componentes', fontsize=12)\n",
    "axes[1].set_ylabel('Varianza Acumulada', fontsize=12)\n",
    "axes[1].set_title('Varianza Acumulada por Componentes', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Para capturar el 80% de la varianza se requieren {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.8) + 1} componentes\")\n",
    "print(f\"âœ“ Para capturar el 90% de la varianza se requieren {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1} componentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 6: MÃ©todo del Codo y MÃ©tricas de ValidaciÃ³n\n",
    "\n",
    "DeterminaciÃ³n del nÃºmero Ã³ptimo de clusters usando mÃºltiples criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MÃ©todo del Codo (Elbow Method) y mÃ©tricas de validaciÃ³n\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACIÃ“N DEL NÃšMERO Ã“PTIMO DE CLUSTERS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nCalculando mÃ©tricas para k=2 hasta k=10...\\n\")\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels_temp))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, labels_temp))\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_score(X_scaled, labels_temp))\n",
    "    \n",
    "    print(f\"k={k}: Inercia={kmeans_temp.inertia_:.2f}, \"\n",
    "          f\"Silhouette={silhouette_scores[-1]:.3f}, \"\n",
    "          f\"Davies-Bouldin={davies_bouldin_scores[-1]:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ CÃ¡lculo completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n de mÃ©tricas de evaluaciÃ³n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('MÃ©tricas de EvaluaciÃ³n para SelecciÃ³n de K Ã“ptimo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# MÃ©todo del Codo (Inertia)\n",
    "axes[0, 0].plot(K_range, inertias, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].set_xlabel('NÃºmero de Clusters (k)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Inercia (Within-Cluster Sum of Squares)', fontsize=11)\n",
    "axes[0, 0].set_title('MÃ©todo del Codo - Inercia', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(K_range)\n",
    "\n",
    "# Silhouette Score (mÃ¡s alto es mejor)\n",
    "axes[0, 1].plot(K_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('NÃºmero de Clusters (k)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0, 1].set_title('Silhouette Score (â†‘ mejor)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(K_range)\n",
    "best_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "axes[0, 1].axvline(x=best_k_silhouette, color='red', linestyle='--', alpha=0.7, label=f'Mejor k={best_k_silhouette}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Davies-Bouldin Index (mÃ¡s bajo es mejor)\n",
    "axes[1, 0].plot(K_range, davies_bouldin_scores, marker='^', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 0].set_xlabel('NÃºmero de Clusters (k)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[1, 0].set_title('Davies-Bouldin Index (â†“ mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(K_range)\n",
    "best_k_db = K_range[np.argmin(davies_bouldin_scores)]\n",
    "axes[1, 0].axvline(x=best_k_db, color='red', linestyle='--', alpha=0.7, label=f'Mejor k={best_k_db}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Calinski-Harabasz Index (mÃ¡s alto es mejor)\n",
    "axes[1, 1].plot(K_range, calinski_harabasz_scores, marker='D', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 1].set_xlabel('NÃºmero de Clusters (k)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Index', fontsize=11)\n",
    "axes[1, 1].set_title('Calinski-Harabasz Index (â†‘ mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(K_range)\n",
    "best_k_ch = K_range[np.argmax(calinski_harabasz_scores)]\n",
    "axes[1, 1].axvline(x=best_k_ch, color='red', linestyle='--', alpha=0.7, label=f'Mejor k={best_k_ch}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMENDACIONES DE K Ã“PTIMO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSilhouette Score recomienda: k = {best_k_silhouette}\")\n",
    "print(f\"Davies-Bouldin Index recomienda: k = {best_k_db}\")\n",
    "print(f\"Calinski-Harabasz Index recomienda: k = {best_k_ch}\")\n",
    "print(f\"\\nâœ“ Considere k={best_k_silhouette} como punto de partida Ã³ptimo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 7: ImplementaciÃ³n de K-Means Clustering\n",
    "\n",
    "AplicaciÃ³n del algoritmo K-Means con el nÃºmero Ã³ptimo de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means con k Ã³ptimo\n",
    "optimal_k = best_k_silhouette  # Puede ajustarse manualmente si se desea\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"APLICANDO K-MEANS CON K={optimal_k}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n",
    "df_clustering['Cluster_KMeans'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\nâœ“ Modelo K-Means entrenado exitosamente\")\n",
    "print(f\"\\nDistribuciÃ³n de clusters:\")\n",
    "cluster_distribution = df_clustering['Cluster_KMeans'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_distribution.items():\n",
    "    percentage = (count / len(df_clustering) * 100)\n",
    "    print(f\"  Cluster {cluster_id}: {count} observaciones ({percentage:.1f}%)\")\n",
    "\n",
    "# MÃ©tricas finales\n",
    "final_silhouette = silhouette_score(X_scaled, df_clustering['Cluster_KMeans'])\n",
    "final_db = davies_bouldin_score(X_scaled, df_clustering['Cluster_KMeans'])\n",
    "final_ch = calinski_harabasz_score(X_scaled, df_clustering['Cluster_KMeans'])\n",
    "\n",
    "print(f\"\\nMÃ©tricas del modelo final:\")\n",
    "print(f\"  Silhouette Score: {final_silhouette:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {final_db:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {final_ch:.2f}\")\n",
    "print(f\"  Inercia: {kmeans_final.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n de clusters en espacio PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle(f'VisualizaciÃ³n de Clusters K-Means (k={optimal_k}) en Espacio PCA', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# PC1 vs PC2\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_KMeans'], \n",
    "                           cmap='viridis', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "axes[0].set_title('Clusters en PC1 vs PC2', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# PC2 vs PC3\n",
    "scatter2 = axes[1].scatter(X_pca[:, 1], X_pca[:, 2], \n",
    "                           c=df_clustering['Cluster_KMeans'], \n",
    "                           cmap='viridis', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Componente Principal 2', fontsize=11)\n",
    "axes[1].set_ylabel('Componente Principal 3', fontsize=11)\n",
    "axes[1].set_title('Clusters en PC2 vs PC3', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 8: Perfilamiento de Clusters\n",
    "\n",
    "AnÃ¡lisis detallado de las caracterÃ­sticas de cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de centroides\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÃLISIS DE CENTROIDES - K-MEANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "centroids_df = pd.DataFrame(\n",
    "    scaler.inverse_transform(kmeans_final.cluster_centers_),\n",
    "    columns=features_for_clustering\n",
    ")\n",
    "centroids_df.index = [f'Cluster {i}' for i in range(optimal_k)]\n",
    "\n",
    "print(\"\\nCentroides de cada cluster (valores originales):\")\n",
    "display(centroids_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfilamiento detallado por cluster\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFILAMIENTO DETALLADO POR CLUSTER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(df_clustering['Cluster_KMeans'].unique()):\n",
    "    cluster_data = df_clustering[df_clustering['Cluster_KMeans'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"CLUSTER {cluster_id} - {len(cluster_data)} observaciones ({len(cluster_data)/len(df_clustering)*100:.1f}%)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\nRating Promedio: {cluster_data['Rating'].mean():.2f} (Â±{cluster_data['Rating'].std():.2f})\")\n",
    "    print(f\"Longitud Promedio de Comentario: {cluster_data['Longitud_Comentario'].mean():.1f} caracteres\")\n",
    "    \n",
    "    print(f\"\\nDistribuciÃ³n de Sentimiento:\")\n",
    "    sentimiento_dist = cluster_data['Sentimiento_Derivado'].value_counts(normalize=True) * 100\n",
    "    for sent, pct in sentimiento_dist.items():\n",
    "        print(f\"  {sent}: {pct:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTop 3 Productos:\")\n",
    "    top_productos = cluster_data['Producto'].value_counts().head(3)\n",
    "    for prod, count in top_productos.items():\n",
    "        print(f\"  {prod}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 3 Canales:\")\n",
    "    top_canales = cluster_data['Canal_Feedback'].value_counts().head(3)\n",
    "    for canal, count in top_canales.items():\n",
    "        print(f\"  {canal}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDistribuciÃ³n por Tipo de Cliente:\")\n",
    "    tipo_dist = cluster_data['Cliente_Tipo'].value_counts()\n",
    "    for tipo, count in tipo_dist.items():\n",
    "        print(f\"  {tipo}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nRegiones principales:\")\n",
    "    region_dist = cluster_data['RegiÃ³n'].value_counts().head(3)\n",
    "    for region, count in region_dist.items():\n",
    "        print(f\"  {region}: {count} ({count/len(cluster_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de caracterÃ­sticas promedio por cluster\n",
    "cluster_profiles = df_clustering.groupby('Cluster_KMeans')[features_for_clustering].mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            linewidths=0.5, cbar_kws={'label': 'Valor Promedio'})\n",
    "plt.title('Perfil de CaracterÃ­sticas por Cluster (K-Means)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('CaracterÃ­stica', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 9: Clustering JerÃ¡rquico\n",
    "\n",
    "AplicaciÃ³n de clustering jerÃ¡rquico aglomerativo para comparaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering JerÃ¡rquico - Dendrograma\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING JERÃRQUICO AGLOMERATIVO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular linkage - usamos una muestra para eficiencia\n",
    "sample_size = min(500, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "print(f\"\\nCalculando dendrograma con {sample_size} observaciones muestreadas...\")\n",
    "\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=20, leaf_font_size=10)\n",
    "plt.title('Dendrograma - Clustering JerÃ¡rquico (Ward)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ãndice de Muestra / TamaÃ±o del Cluster', fontsize=12)\n",
    "plt.ylabel('Distancia', fontsize=12)\n",
    "plt.axhline(y=50, color='red', linestyle='--', label='Corte sugerido')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Dendrograma generado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar clustering jerÃ¡rquico al dataset completo\n",
    "n_clusters_hierarchical = optimal_k\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters_hierarchical, linkage='ward')\n",
    "df_clustering['Cluster_Hierarchical'] = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"CLUSTERING JERÃRQUICO CON k={n_clusters_hierarchical}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nâœ“ Modelo aplicado exitosamente\")\n",
    "print(f\"\\nDistribuciÃ³n de clusters jerÃ¡rquicos:\")\n",
    "hierarchical_distribution = df_clustering['Cluster_Hierarchical'].value_counts().sort_index()\n",
    "for cluster_id, count in hierarchical_distribution.items():\n",
    "    percentage = (count / len(df_clustering) * 100)\n",
    "    print(f\"  Cluster {cluster_id}: {count} observaciones ({percentage:.1f}%)\")\n",
    "\n",
    "# MÃ©tricas\n",
    "hierarchical_silhouette = silhouette_score(X_scaled, df_clustering['Cluster_Hierarchical'])\n",
    "hierarchical_db = davies_bouldin_score(X_scaled, df_clustering['Cluster_Hierarchical'])\n",
    "hierarchical_ch = calinski_harabasz_score(X_scaled, df_clustering['Cluster_Hierarchical'])\n",
    "\n",
    "print(f\"\\nMÃ©tricas del modelo jerÃ¡rquico:\")\n",
    "print(f\"  Silhouette Score: {hierarchical_silhouette:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {hierarchical_db:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {hierarchical_ch:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n de clusters jerÃ¡rquicos en PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle(f'VisualizaciÃ³n de Clustering JerÃ¡rquico (k={n_clusters_hierarchical}) en Espacio PCA', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_Hierarchical'], \n",
    "                           cmap='plasma', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "axes[0].set_title('Clusters JerÃ¡rquicos en PC1 vs PC2', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "scatter2 = axes[1].scatter(X_pca[:, 1], X_pca[:, 2], \n",
    "                           c=df_clustering['Cluster_Hierarchical'], \n",
    "                           cmap='plasma', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Componente Principal 2', fontsize=11)\n",
    "axes[1].set_ylabel('Componente Principal 3', fontsize=11)\n",
    "axes[1].set_title('Clusters JerÃ¡rquicos en PC2 vs PC3', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 10: DBSCAN - Density-Based Clustering\n",
    "\n",
    "AplicaciÃ³n de DBSCAN para identificar clusters de densidad variable y outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "print(\"=\" * 80)\n",
    "print(\"DBSCAN - DENSITY-BASED SPATIAL CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Probar diferentes valores de epsilon\n",
    "print(\"\\nBuscando parÃ¡metros Ã³ptimos de DBSCAN...\")\n",
    "\n",
    "dbscan = DBSCAN(eps=2.0, min_samples=10)\n",
    "df_clustering['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(df_clustering['Cluster_DBSCAN'])) - (1 if -1 in df_clustering['Cluster_DBSCAN'] else 0)\n",
    "n_noise = list(df_clustering['Cluster_DBSCAN']).count(-1)\n",
    "\n",
    "print(f\"\\nâœ“ DBSCAN aplicado con eps=2.0, min_samples=10\")\n",
    "print(f\"\\nResultados:\")\n",
    "print(f\"  NÃºmero de clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"  Puntos clasificados como ruido (outliers): {n_noise} ({n_noise/len(df_clustering)*100:.1f}%)\")\n",
    "\n",
    "if n_clusters_dbscan > 0:\n",
    "    print(f\"\\nDistribuciÃ³n de clusters DBSCAN:\")\n",
    "    dbscan_distribution = df_clustering[df_clustering['Cluster_DBSCAN'] != -1]['Cluster_DBSCAN'].value_counts().sort_index()\n",
    "    for cluster_id, count in dbscan_distribution.items():\n",
    "        percentage = (count / len(df_clustering) * 100)\n",
    "        print(f\"  Cluster {cluster_id}: {count} observaciones ({percentage:.1f}%)\")\n",
    "\n",
    "    # MÃ©tricas (excluyendo ruido)\n",
    "    df_dbscan_no_noise = df_clustering[df_clustering['Cluster_DBSCAN'] != -1]\n",
    "    X_dbscan_no_noise = X_scaled[df_clustering['Cluster_DBSCAN'] != -1]\n",
    "    \n",
    "    if len(df_dbscan_no_noise) > 0 and n_clusters_dbscan > 1:\n",
    "        dbscan_silhouette = silhouette_score(X_dbscan_no_noise, df_dbscan_no_noise['Cluster_DBSCAN'])\n",
    "        dbscan_db = davies_bouldin_score(X_dbscan_no_noise, df_dbscan_no_noise['Cluster_DBSCAN'])\n",
    "        dbscan_ch = calinski_harabasz_score(X_dbscan_no_noise, df_dbscan_no_noise['Cluster_DBSCAN'])\n",
    "        \n",
    "        print(f\"\\nMÃ©tricas del modelo DBSCAN (sin ruido):\")\n",
    "        print(f\"  Silhouette Score: {dbscan_silhouette:.4f}\")\n",
    "        print(f\"  Davies-Bouldin Index: {dbscan_db:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz Index: {dbscan_ch:.2f}\")\n",
    "else:\n",
    "    print(\"\\nâš  DBSCAN no encontrÃ³ clusters significativos con estos parÃ¡metros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n DBSCAN\n",
    "if n_clusters_dbscan > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    fig.suptitle(f'VisualizaciÃ³n de DBSCAN ({n_clusters_dbscan} clusters, {n_noise} outliers) en Espacio PCA', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Crear colormap personalizado con color especial para outliers\n",
    "    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                               c=df_clustering['Cluster_DBSCAN'], \n",
    "                               cmap='tab10', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    axes[0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "    axes[0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "    axes[0].set_title('DBSCAN en PC1 vs PC2 (ruido en -1)', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    \n",
    "    scatter2 = axes[1].scatter(X_pca[:, 1], X_pca[:, 2], \n",
    "                               c=df_clustering['Cluster_DBSCAN'], \n",
    "                               cmap='tab10', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    axes[1].set_xlabel('Componente Principal 2', fontsize=11)\n",
    "    axes[1].set_ylabel('Componente Principal 3', fontsize=11)\n",
    "    axes[1].set_title('DBSCAN en PC2 vs PC3 (ruido en -1)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 11: ComparaciÃ³n de MÃ©todos de Clustering\n",
    "\n",
    "AnÃ¡lisis comparativo de los tres algoritmos implementados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa de mÃ©tricas\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACIÃ“N DE MÃ‰TODOS DE CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = {\n",
    "    'MÃ©todo': ['K-Means', 'JerÃ¡rquico', 'DBSCAN'],\n",
    "    'NÂ° Clusters': [\n",
    "        optimal_k,\n",
    "        n_clusters_hierarchical,\n",
    "        n_clusters_dbscan\n",
    "    ],\n",
    "    'Silhouette Score': [\n",
    "        final_silhouette,\n",
    "        hierarchical_silhouette,\n",
    "        dbscan_silhouette if n_clusters_dbscan > 1 else np.nan\n",
    "    ],\n",
    "    'Davies-Bouldin Index': [\n",
    "        final_db,\n",
    "        hierarchical_db,\n",
    "        dbscan_db if n_clusters_dbscan > 1 else np.nan\n",
    "    ],\n",
    "    'Calinski-Harabasz Index': [\n",
    "        final_ch,\n",
    "        hierarchical_ch,\n",
    "        dbscan_ch if n_clusters_dbscan > 1 else np.nan\n",
    "    ],\n",
    "    'Outliers Detectados': [\n",
    "        0,\n",
    "        0,\n",
    "        n_noise\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nInterpretaciÃ³n de mÃ©tricas:\")\n",
    "print(\"  â€¢ Silhouette Score: MÃ¡s alto es mejor (rango: -1 a 1)\")\n",
    "print(\"  â€¢ Davies-Bouldin Index: MÃ¡s bajo es mejor (rango: 0 a âˆ)\")\n",
    "print(\"  â€¢ Calinski-Harabasz Index: MÃ¡s alto es mejor (rango: 0 a âˆ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n comparativa\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('ComparaciÃ³n Visual de MÃ©todos de Clustering en PC1 vs PC2', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_KMeans'], \n",
    "                           cmap='viridis', s=40, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "axes[0].set_title(f'K-Means (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# JerÃ¡rquico\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_Hierarchical'], \n",
    "                           cmap='plasma', s=40, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "axes[1].set_title(f'JerÃ¡rquico (k={n_clusters_hierarchical})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "# DBSCAN\n",
    "scatter3 = axes[2].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_DBSCAN'], \n",
    "                           cmap='tab10', s=40, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "axes[2].set_title(f'DBSCAN ({n_clusters_dbscan} clusters, {n_noise} outliers)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('PC1')\n",
    "axes[2].set_ylabel('PC2')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 12: InterpretaciÃ³n de Negocio - Insights Accionables\n",
    "\n",
    "TraducciÃ³n de resultados tÃ©cnicos a recomendaciones estratÃ©gicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de palabras clave por cluster (K-Means)\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÃLISIS DE PALABRAS CLAVE POR CLUSTER (K-MEANS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(df_clustering['Cluster_KMeans'].unique()):\n",
    "    cluster_comments = df_clustering[df_clustering['Cluster_KMeans'] == cluster_id]['Comentario']\n",
    "    \n",
    "    # Unir todos los comentarios\n",
    "    all_text = ' '.join(cluster_comments.astype(str))\n",
    "    \n",
    "    # TokenizaciÃ³n simple\n",
    "    words = all_text.lower().split()\n",
    "    \n",
    "    # Filtrar palabras comunes (stopwords bÃ¡sico)\n",
    "    stopwords = ['el', 'la', 'de', 'y', 'a', 'en', 'es', 'por', 'con', 'los', 'las',\n",
    "                 'del', 'un', 'una', 'para', 'al', 'que', 'se', 'su', 'me', 'mi',\n",
    "                 'muy', 'pero', 'mÃ¡s', 'son', 'como', 'lo', 'le', 'ha']\n",
    "    words_filtered = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    \n",
    "    # Contar frecuencias\n",
    "    word_freq = Counter(words_filtered)\n",
    "    top_words = word_freq.most_common(10)\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for word, freq in top_words:\n",
    "        print(f\"  {word}: {freq} menciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard de insights por cluster\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN EJECUTIVO - INSIGHTS POR CLUSTER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(df_clustering['Cluster_KMeans'].unique()):\n",
    "    cluster_data = df_clustering[df_clustering['Cluster_KMeans'] == cluster_id]\n",
    "    \n",
    "    # Calcular mÃ©tricas clave\n",
    "    avg_rating = cluster_data['Rating'].mean()\n",
    "    size = len(cluster_data)\n",
    "    size_pct = size / len(df_clustering) * 100\n",
    "    sentiment_mode = cluster_data['Sentimiento_Derivado'].mode()[0]\n",
    "    top_product = cluster_data['Producto'].mode()[0]\n",
    "    top_channel = cluster_data['Canal_Feedback'].mode()[0]\n",
    "    new_clients_pct = (cluster_data['Cliente_Tipo'] == 'Nuevo').sum() / len(cluster_data) * 100\n",
    "    \n",
    "    print(f\"\\nâ•”{'â•' * 78}â•—\")\n",
    "    print(f\"â•‘ CLUSTER {cluster_id}: {sentiment_mode.upper()} - {size} clientes ({size_pct:.1f}%)\" + \" \" * (78 - len(f\" CLUSTER {cluster_id}: {sentiment_mode.upper()} - {size} clientes ({size_pct:.1f}%)\") - 1) + \"â•‘\")\n",
    "    print(f\"â• {'â•' * 78}â•£\")\n",
    "    \n",
    "    # Perfil\n",
    "    print(f\"â•‘ ğŸ“Š Rating Promedio: {avg_rating:.2f}/5.0\" + \" \" * (78 - len(f\" ğŸ“Š Rating Promedio: {avg_rating:.2f}/5.0\") - 1) + \"â•‘\")\n",
    "    print(f\"â•‘ ğŸ¯ Producto Principal: {top_product}\" + \" \" * (78 - len(f\" ğŸ¯ Producto Principal: {top_product}\") - 1) + \"â•‘\")\n",
    "    print(f\"â•‘ ğŸ“± Canal Predominante: {top_channel}\" + \" \" * (78 - len(f\" ğŸ“± Canal Predominante: {top_channel}\") - 1) + \"â•‘\")\n",
    "    print(f\"â•‘ ğŸ†• Clientes Nuevos: {new_clients_pct:.1f}%\" + \" \" * (78 - len(f\" ğŸ†• Clientes Nuevos: {new_clients_pct:.1f}%\") - 1) + \"â•‘\")\n",
    "    \n",
    "    # CaracterizaciÃ³n\n",
    "    if avg_rating >= 4.0:\n",
    "        caracterizacion = \"Clientes SATISFECHOS - Prioridad: RETENCIÃ“N\"\n",
    "    elif avg_rating >= 3.0:\n",
    "        caracterizacion = \"Clientes NEUTRALES - Prioridad: MEJORA CONTINUA\"\n",
    "    else:\n",
    "        caracterizacion = \"Clientes EN RIESGO - Prioridad: INTERVENCIÃ“N INMEDIATA\"\n",
    "    \n",
    "    print(f\"â•‘ âš ï¸ {caracterizacion}\" + \" \" * (78 - len(f\" âš ï¸ {caracterizacion}\") - 1) + \"â•‘\")\n",
    "    print(f\"â•š{'â•' * 78}â•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 13: ExportaciÃ³n de Resultados\n",
    "\n",
    "GeneraciÃ³n de datasets con asignaciones de clusters para anÃ¡lisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar dataset completo con clusters asignados\n",
    "output_columns = [\n",
    "    'ID_Comentario', 'Fecha', 'Producto', 'Canal_Feedback', 'Comentario',\n",
    "    'Rating', 'Cliente_Tipo', 'RegiÃ³n', \n",
    "    'Cluster_KMeans', 'Cluster_Hierarchical', 'Cluster_DBSCAN',\n",
    "    'Sentimiento_Derivado', 'Longitud_Comentario', 'Num_Palabras'\n",
    "]\n",
    "\n",
    "df_export = df_clustering[output_columns].copy()\n",
    "\n",
    "# Guardar a CSV\n",
    "output_filename = 'feedback_con_clusters.csv'\n",
    "df_export.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTACIÃ“N DE RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ“ Dataset exportado exitosamente: {output_filename}\")\n",
    "print(f\"  Dimensiones: {df_export.shape[0]} filas x {df_export.shape[1]} columnas\")\n",
    "print(f\"\\nColumnas exportadas:\")\n",
    "for col in output_columns:\n",
    "    print(f\"  â€¢ {col}\")\n",
    "\n",
    "# Resumen estadÃ­stico del export\n",
    "print(\"\\nPrimeras 10 filas del dataset exportado:\")\n",
    "display(df_export.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 14: Conclusiones y Recomendaciones EstratÃ©gicas\n",
    "\n",
    "SÃ­ntesis ejecutiva para stakeholders y prÃ³ximos pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES ESTRATÃ‰GICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                           RESUMEN EJECUTIVO                                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                                â•‘\n",
    "â•‘  1. HALLAZGOS PRINCIPALES                                                     â•‘\n",
    "â•‘     â€¢ Se identificaron {} clusters significativos de clientes                 â•‘\n",
    "â•‘     â€¢ K-Means demostrÃ³ ser el mÃ©todo mÃ¡s balanceado (Silhouette: {:.3f})      â•‘\n",
    "â•‘     â€¢ Se detectaron {} outliers mediante DBSCAN                               â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•‘  2. SEGMENTOS CLAVE IDENTIFICADOS                                             â•‘\n",
    "â•‘     â€¢ Analizar cada cluster para entender patrones de satisfacciÃ³n           â•‘\n",
    "â•‘     â€¢ Priorizar acciones en clusters con bajo rating                          â•‘\n",
    "â•‘     â€¢ Replicar prÃ¡cticas exitosas de clusters con alto rating                â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•‘  3. RECOMENDACIONES TÃCTICAS                                                  â•‘\n",
    "â•‘     âœ“ IntervenciÃ³n inmediata en clusters con Rating < 3.0                     â•‘\n",
    "â•‘     âœ“ Programas de fidelizaciÃ³n para clusters de alto valor                  â•‘\n",
    "â•‘     âœ“ OptimizaciÃ³n de canales segÃºn preferencias por cluster                 â•‘\n",
    "â•‘     âœ“ PersonalizaciÃ³n de productos por segmento                               â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•‘  4. PRÃ“XIMOS PASOS                                                            â•‘\n",
    "â•‘     â†’ AnÃ¡lisis de sentimiento textual con NLP avanzado                        â•‘\n",
    "â•‘     â†’ Modelos predictivos de churn por cluster                                â•‘\n",
    "â•‘     â†’ CÃ¡lculo de Customer Lifetime Value (CLV) segmentado                     â•‘\n",
    "â•‘     â†’ Dashboards interactivos para monitoreo continuo                         â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\".format(\n",
    "    optimal_k,\n",
    "    final_silhouette,\n",
    "    n_noise\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EJERCICIO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ“ Todos los bloques ejecutados\")\n",
    "print(\"âœ“ Modelos de clustering entrenados y evaluados\")\n",
    "print(\"âœ“ Insights de negocio generados\")\n",
    "print(\"âœ“ Resultados exportados\")\n",
    "print(\"\\nEl anÃ¡lisis estÃ¡ listo para ser presentado a stakeholders.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
