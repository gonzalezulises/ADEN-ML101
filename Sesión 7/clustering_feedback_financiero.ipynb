{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Clustering No Supervisado: Segmentación de Feedback Financiero\n",
    "\n",
    "## Objetivo del Ejercicio\n",
    "\n",
    "Aplicar técnicas de **Machine Learning No Supervisado** para identificar patrones y segmentos naturales en el feedback de clientes de productos financieros. Este análisis permite:\n",
    "\n",
    "- Identificar grupos de clientes con comportamientos y experiencias similares\n",
    "- Descubrir insights ocultos en datos no etiquetados\n",
    "- Priorizar acciones de mejora basadas en segmentos críticos\n",
    "- Generar hipótesis para análisis supervisados posteriores\n",
    "\n",
    "## Estructura del Notebook\n",
    "\n",
    "1. **Configuración del Entorno**\n",
    "2. **Exploración y Preparación de Datos**\n",
    "3. **Ingeniería de Características**\n",
    "4. **Análisis de Clustering K-Means**\n",
    "5. **Clustering Jerárquico**\n",
    "6. **DBSCAN - Density-Based Clustering**\n",
    "7. **Comparación de Modelos**\n",
    "8. **Interpretación de Negocio**\n",
    "9. **Recomendaciones Estratégicas**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE 1: Configuración del Entorno y Librerías\n",
    "\n",
    "Instalación y carga de todas las dependencias necesarias para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configuración de visualización\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuración de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✓ Librerías básicas cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías de Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Librerías de procesamiento de texto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "print(\"✓ Librerías de ML y NLP cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 2: Carga y Exploración Inicial de Datos\n",
    "\n",
    "Análisis exploratorio para comprender la estructura y calidad del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "df = pd.read_csv('financial_feedback_data.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "print(f\"\\nPrimeras 5 observaciones:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INFORMACIÓN DE TIPOS DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores faltantes\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Valores_Faltantes': df.isnull().sum(),\n",
    "    'Porcentaje': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Valores_Faltantes'] > 0].sort_values('Valores_Faltantes', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    display(missing_data)\n",
    "else:\n",
    "    print(\"✓ No se detectaron valores faltantes en el dataset\")\n",
    "\n",
    "# Estadísticas descriptivas de variables numéricas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS - VARIABLES NUMÉRICAS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de variables categóricas\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUCIÓN DE VARIABLES CATEGÓRICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_cols = ['Producto', 'Canal_Feedback', 'Cliente_Tipo', 'Región']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(\"-\" * 40)\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentage = (value_counts / len(df) * 100).round(2)\n",
    "    result = pd.DataFrame({\n",
    "        'Frecuencia': value_counts,\n",
    "        'Porcentaje': percentage\n",
    "    })\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 3: Análisis Exploratorio Visual\n",
    "\n",
    "Visualizaciones para identificar patrones preliminares en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de Rating\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Análisis de Distribución de Variables Clave', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rating distribution\n",
    "sns.countplot(data=df, x='Rating', ax=axes[0, 0], palette='viridis')\n",
    "axes[0, 0].set_title('Distribución de Rating', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Rating')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for container in axes[0, 0].containers:\n",
    "    axes[0, 0].bar_label(container)\n",
    "\n",
    "# Producto distribution\n",
    "product_counts = df['Producto'].value_counts()\n",
    "axes[0, 1].barh(product_counts.index, product_counts.values, color='coral')\n",
    "axes[0, 1].set_title('Distribución por Producto', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Frecuencia')\n",
    "axes[0, 1].set_ylabel('Producto')\n",
    "\n",
    "# Canal Feedback distribution\n",
    "canal_counts = df['Canal_Feedback'].value_counts()\n",
    "axes[1, 0].barh(canal_counts.index, canal_counts.values, color='lightblue')\n",
    "axes[1, 0].set_title('Distribución por Canal de Feedback', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Frecuencia')\n",
    "axes[1, 0].set_ylabel('Canal')\n",
    "\n",
    "# Región distribution\n",
    "sns.countplot(data=df, x='Región', ax=axes[1, 1], palette='Set2')\n",
    "axes[1, 1].set_title('Distribución por Región', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Región')\n",
    "axes[1, 1].set_ylabel('Frecuencia')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for container in axes[1, 1].containers:\n",
    "    axes[1, 1].bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de Rating por Producto y Canal\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle('Rating Promedio por Categorías', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rating por Producto\n",
    "rating_por_producto = df.groupby('Producto')['Rating'].mean().sort_values(ascending=True)\n",
    "axes[0].barh(rating_por_producto.index, rating_por_producto.values, color='steelblue')\n",
    "axes[0].set_title('Rating Promedio por Producto', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Rating Promedio')\n",
    "axes[0].set_xlim(0, 5)\n",
    "axes[0].axvline(x=df['Rating'].mean(), color='red', linestyle='--', label=f'Media Global: {df[\"Rating\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Rating por Canal\n",
    "rating_por_canal = df.groupby('Canal_Feedback')['Rating'].mean().sort_values(ascending=True)\n",
    "axes[1].barh(rating_por_canal.index, rating_por_canal.values, color='darkorange')\n",
    "axes[1].set_title('Rating Promedio por Canal de Feedback', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Rating Promedio')\n",
    "axes[1].set_xlim(0, 5)\n",
    "axes[1].axvline(x=df['Rating'].mean(), color='red', linestyle='--', label=f'Media Global: {df[\"Rating\"].mean():.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 4: Ingeniería de Características\n",
    "\n",
    "Transformación de variables categóricas y creación de features numéricas para clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataframe para no modificar el original\n",
    "df_clustering = df.copy()\n",
    "\n",
    "# Convertir fecha a datetime y extraer características temporales\n",
    "df_clustering['Fecha'] = pd.to_datetime(df_clustering['Fecha'])\n",
    "df_clustering['Mes'] = df_clustering['Fecha'].dt.month\n",
    "df_clustering['Trimestre'] = df_clustering['Fecha'].dt.quarter\n",
    "df_clustering['Dia_Semana'] = df_clustering['Fecha'].dt.dayofweek\n",
    "\n",
    "# Análisis de longitud de comentarios\n",
    "df_clustering['Longitud_Comentario'] = df_clustering['Comentario'].str.len()\n",
    "df_clustering['Num_Palabras'] = df_clustering['Comentario'].str.split().str.len()\n",
    "\n",
    "# Clasificación manual de sentimiento basado en Rating\n",
    "def clasificar_sentimiento(rating):\n",
    "    if rating <= 2:\n",
    "        return 'Negativo'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positivo'\n",
    "\n",
    "df_clustering['Sentimiento_Derivado'] = df_clustering['Rating'].apply(clasificar_sentimiento)\n",
    "\n",
    "print(\"✓ Características temporales y de texto creadas\")\n",
    "print(f\"\\nNuevas características:\")\n",
    "print(f\"  - Mes, Trimestre, Dia_Semana\")\n",
    "print(f\"  - Longitud_Comentario, Num_Palabras\")\n",
    "print(f\"  - Sentimiento_Derivado\")\n",
    "\n",
    "display(df_clustering[['ID_Comentario', 'Rating', 'Mes', 'Trimestre', \n",
    "                        'Longitud_Comentario', 'Num_Palabras', 'Sentimiento_Derivado']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding de variables categóricas\n",
    "print(\"=\" * 80)\n",
    "print(\"ENCODING DE VARIABLES CATEGÓRICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Label Encoding para variables con cardinalidad baja\n",
    "le_dict = {}\n",
    "categorical_features = ['Producto', 'Canal_Feedback', 'Cliente_Tipo', 'Región', 'Sentimiento_Derivado']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_clustering[f'{feature}_Encoded'] = le.fit_transform(df_clustering[feature])\n",
    "    le_dict[feature] = le\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Clases: {list(le.classes_)}\")\n",
    "    print(f\"  Encodings: {list(range(len(le.classes_)))}\")\n",
    "\n",
    "print(\"\\n✓ Variables categóricas codificadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de features para clustering\n",
    "features_for_clustering = [\n",
    "    'Rating',\n",
    "    'Producto_Encoded',\n",
    "    'Canal_Feedback_Encoded',\n",
    "    'Cliente_Tipo_Encoded',\n",
    "    'Región_Encoded',\n",
    "    'Mes',\n",
    "    'Trimestre',\n",
    "    'Longitud_Comentario',\n",
    "    'Num_Palabras',\n",
    "    'Sentimiento_Derivado_Encoded'\n",
    "]\n",
    "\n",
    "X = df_clustering[features_for_clustering].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MATRIZ DE CARACTERÍSTICAS PARA CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDimensiones: {X.shape}\")\n",
    "print(f\"\\nFeatures seleccionadas:\")\n",
    "for i, feat in enumerate(features_for_clustering, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\nEstadísticas de la matriz de features:\")\n",
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_clustering)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTANDARIZACIÓN DE CARACTERÍSTICAS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ Features estandarizadas con StandardScaler (media=0, std=1)\")\n",
    "print(f\"\\nDimensiones post-escalamiento: {X_scaled.shape}\")\n",
    "print(\"\\nPrimeras 5 observaciones estandarizadas:\")\n",
    "display(X_scaled_df.head())\n",
    "\n",
    "print(\"\\nValidación de estandarización:\")\n",
    "print(f\"  Media de features: {X_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"  Desv. Est. de features: {X_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 5: Reducción Dimensional con PCA\n",
    "\n",
    "Aplicación de PCA para visualización y análisis de varianza explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA para visualización\n",
    "pca = PCA(n_components=min(10, X_scaled.shape[1]))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE COMPONENTES PRINCIPALES (PCA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Varianza explicada\n",
    "variance_df = pd.DataFrame({\n",
    "    'Componente': [f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))],\n",
    "    'Varianza_Explicada': pca.explained_variance_ratio_,\n",
    "    'Varianza_Acumulada': np.cumsum(pca.explained_variance_ratio_)\n",
    "})\n",
    "\n",
    "print(\"\\nVarianza explicada por componente:\")\n",
    "display(variance_df)\n",
    "\n",
    "# Visualización de varianza explicada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "            pca.explained_variance_ratio_, \n",
    "            color='steelblue', alpha=0.7)\n",
    "axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             pca.explained_variance_ratio_, \n",
    "             marker='o', color='red', linewidth=2)\n",
    "axes[0].set_xlabel('Componente Principal', fontsize=12)\n",
    "axes[0].set_ylabel('Varianza Explicada', fontsize=12)\n",
    "axes[0].set_title('Scree Plot - Varianza Explicada por Componente', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Varianza acumulada\n",
    "axes[1].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             np.cumsum(pca.explained_variance_ratio_), \n",
    "             marker='o', linewidth=2, color='darkgreen')\n",
    "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% Varianza')\n",
    "axes[1].axhline(y=0.9, color='orange', linestyle='--', label='90% Varianza')\n",
    "axes[1].set_xlabel('Número de Componentes', fontsize=12)\n",
    "axes[1].set_ylabel('Varianza Acumulada', fontsize=12)\n",
    "axes[1].set_title('Varianza Acumulada por Componentes', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Para capturar el 80% de la varianza se requieren {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.8) + 1} componentes\")\n",
    "print(f\"✓ Para capturar el 90% de la varianza se requieren {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1} componentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 6: Método del Codo y Métricas de Validación\n",
    "\n",
    "Determinación del número óptimo de clusters usando múltiples criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método del Codo (Elbow Method) y métricas de validación\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACIÓN DEL NÚMERO ÓPTIMO DE CLUSTERS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nCalculando métricas para k=2 hasta k=10...\\n\")\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels_temp))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, labels_temp))\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_score(X_scaled, labels_temp))\n",
    "    \n",
    "    print(f\"k={k}: Inercia={kmeans_temp.inertia_:.2f}, \"\n",
    "          f\"Silhouette={silhouette_scores[-1]:.3f}, \"\n",
    "          f\"Davies-Bouldin={davies_bouldin_scores[-1]:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Cálculo completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de métricas de evaluación\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Métricas de Evaluación para Selección de K Óptimo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Método del Codo (Inertia)\n",
    "axes[0, 0].plot(K_range, inertias, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Número de Clusters (k)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Inercia (Within-Cluster Sum of Squares)', fontsize=11)\n",
    "axes[0, 0].set_title('Método del Codo - Inercia', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(K_range)\n",
    "\n",
    "# Silhouette Score (más alto es mejor)\n",
    "axes[0, 1].plot(K_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('Número de Clusters (k)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0, 1].set_title('Silhouette Score (↑ mejor)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(K_range)\n",
    "best_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "axes[0, 1].axvline(x=best_k_silhouette, color='red', linestyle='--', alpha=0.7, label=f'Mejor k={best_k_silhouette}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Davies-Bouldin Index (más bajo es mejor)\n",
    "axes[1, 0].plot(K_range, davies_bouldin_scores, marker='^', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 0].set_xlabel('Número de Clusters (k)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[1, 0].set_title('Davies-Bouldin Index (↓ mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(K_range)\n",
    "best_k_db = K_range[np.argmin(davies_bouldin_scores)]\n",
    "axes[1, 0].axvline(x=best_k_db, color='red', linestyle='--', alpha=0.7, label=f'Mejor k={best_k_db}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Calinski-Harabasz Index (más alto es mejor)\n",
    "axes[1, 1].plot(K_range, calinski_harabasz_scores, marker='D', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 1].set_xlabel('Número de Clusters (k)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Index', fontsize=11)\n",
    "axes[1, 1].set_title('Calinski-Harabasz Index (↑ mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(K_range)\n",
    "best_k_ch = K_range[np.argmax(calinski_harabasz_scores)]\n",
    "axes[1, 1].axvline(x=best_k_ch, color='red', linestyle='--', alpha=0.7, label=f'Mejor k={best_k_ch}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMENDACIONES DE K ÓPTIMO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSilhouette Score recomienda: k = {best_k_silhouette}\")\n",
    "print(f\"Davies-Bouldin Index recomienda: k = {best_k_db}\")\n",
    "print(f\"Calinski-Harabasz Index recomienda: k = {best_k_ch}\")\n",
    "print(f\"\\n✓ Considere k={best_k_silhouette} como punto de partida óptimo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 7: Implementación de K-Means Clustering\n",
    "\n",
    "Aplicación del algoritmo K-Means con el número óptimo de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means con k óptimo\n",
    "optimal_k = best_k_silhouette  # Puede ajustarse manualmente si se desea\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"APLICANDO K-MEANS CON K={optimal_k}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n",
    "df_clustering['Cluster_KMeans'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\n✓ Modelo K-Means entrenado exitosamente\")\n",
    "print(f\"\\nDistribución de clusters:\")\n",
    "cluster_distribution = df_clustering['Cluster_KMeans'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_distribution.items():\n",
    "    percentage = (count / len(df_clustering) * 100)\n",
    "    print(f\"  Cluster {cluster_id}: {count} observaciones ({percentage:.1f}%)\")\n",
    "\n",
    "# Métricas finales\n",
    "final_silhouette = silhouette_score(X_scaled, df_clustering['Cluster_KMeans'])\n",
    "final_db = davies_bouldin_score(X_scaled, df_clustering['Cluster_KMeans'])\n",
    "final_ch = calinski_harabasz_score(X_scaled, df_clustering['Cluster_KMeans'])\n",
    "\n",
    "print(f\"\\nMétricas del modelo final:\")\n",
    "print(f\"  Silhouette Score: {final_silhouette:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {final_db:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {final_ch:.2f}\")\n",
    "print(f\"  Inercia: {kmeans_final.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de clusters en espacio PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle(f'Visualización de Clusters K-Means (k={optimal_k}) en Espacio PCA', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# PC1 vs PC2\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_KMeans'], \n",
    "                           cmap='viridis', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "axes[0].set_title('Clusters en PC1 vs PC2', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# PC2 vs PC3\n",
    "scatter2 = axes[1].scatter(X_pca[:, 1], X_pca[:, 2], \n",
    "                           c=df_clustering['Cluster_KMeans'], \n",
    "                           cmap='viridis', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Componente Principal 2', fontsize=11)\n",
    "axes[1].set_ylabel('Componente Principal 3', fontsize=11)\n",
    "axes[1].set_title('Clusters en PC2 vs PC3', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 8: Perfilamiento de Clusters\n",
    "\n",
    "Análisis detallado de las características de cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de centroides\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE CENTROIDES - K-MEANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "centroids_df = pd.DataFrame(\n",
    "    scaler.inverse_transform(kmeans_final.cluster_centers_),\n",
    "    columns=features_for_clustering\n",
    ")\n",
    "centroids_df.index = [f'Cluster {i}' for i in range(optimal_k)]\n",
    "\n",
    "print(\"\\nCentroides de cada cluster (valores originales):\")\n",
    "display(centroids_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfilamiento detallado por cluster\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFILAMIENTO DETALLADO POR CLUSTER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(df_clustering['Cluster_KMeans'].unique()):\n",
    "    cluster_data = df_clustering[df_clustering['Cluster_KMeans'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"CLUSTER {cluster_id} - {len(cluster_data)} observaciones ({len(cluster_data)/len(df_clustering)*100:.1f}%)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\nRating Promedio: {cluster_data['Rating'].mean():.2f} (±{cluster_data['Rating'].std():.2f})\")\n",
    "    print(f\"Longitud Promedio de Comentario: {cluster_data['Longitud_Comentario'].mean():.1f} caracteres\")\n",
    "    \n",
    "    print(f\"\\nDistribución de Sentimiento:\")\n",
    "    sentimiento_dist = cluster_data['Sentimiento_Derivado'].value_counts(normalize=True) * 100\n",
    "    for sent, pct in sentimiento_dist.items():\n",
    "        print(f\"  {sent}: {pct:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTop 3 Productos:\")\n",
    "    top_productos = cluster_data['Producto'].value_counts().head(3)\n",
    "    for prod, count in top_productos.items():\n",
    "        print(f\"  {prod}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 3 Canales:\")\n",
    "    top_canales = cluster_data['Canal_Feedback'].value_counts().head(3)\n",
    "    for canal, count in top_canales.items():\n",
    "        print(f\"  {canal}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDistribución por Tipo de Cliente:\")\n",
    "    tipo_dist = cluster_data['Cliente_Tipo'].value_counts()\n",
    "    for tipo, count in tipo_dist.items():\n",
    "        print(f\"  {tipo}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nRegiones principales:\")\n",
    "    region_dist = cluster_data['Región'].value_counts().head(3)\n",
    "    for region, count in region_dist.items():\n",
    "        print(f\"  {region}: {count} ({count/len(cluster_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de características promedio por cluster\n",
    "cluster_profiles = df_clustering.groupby('Cluster_KMeans')[features_for_clustering].mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            linewidths=0.5, cbar_kws={'label': 'Valor Promedio'})\n",
    "plt.title('Perfil de Características por Cluster (K-Means)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Característica', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 9: Clustering Jerárquico\n",
    "\n",
    "Aplicación de clustering jerárquico aglomerativo para comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Jerárquico - Dendrograma\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING JERÁRQUICO AGLOMERATIVO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular linkage - usamos una muestra para eficiencia\n",
    "sample_size = min(500, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "print(f\"\\nCalculando dendrograma con {sample_size} observaciones muestreadas...\")\n",
    "\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=20, leaf_font_size=10)\n",
    "plt.title('Dendrograma - Clustering Jerárquico (Ward)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Índice de Muestra / Tamaño del Cluster', fontsize=12)\n",
    "plt.ylabel('Distancia', fontsize=12)\n",
    "plt.axhline(y=50, color='red', linestyle='--', label='Corte sugerido')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Dendrograma generado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar clustering jerárquico al dataset completo\n",
    "n_clusters_hierarchical = optimal_k\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters_hierarchical, linkage='ward')\n",
    "df_clustering['Cluster_Hierarchical'] = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"CLUSTERING JERÁRQUICO CON k={n_clusters_hierarchical}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✓ Modelo aplicado exitosamente\")\n",
    "print(f\"\\nDistribución de clusters jerárquicos:\")\n",
    "hierarchical_distribution = df_clustering['Cluster_Hierarchical'].value_counts().sort_index()\n",
    "for cluster_id, count in hierarchical_distribution.items():\n",
    "    percentage = (count / len(df_clustering) * 100)\n",
    "    print(f\"  Cluster {cluster_id}: {count} observaciones ({percentage:.1f}%)\")\n",
    "\n",
    "# Métricas\n",
    "hierarchical_silhouette = silhouette_score(X_scaled, df_clustering['Cluster_Hierarchical'])\n",
    "hierarchical_db = davies_bouldin_score(X_scaled, df_clustering['Cluster_Hierarchical'])\n",
    "hierarchical_ch = calinski_harabasz_score(X_scaled, df_clustering['Cluster_Hierarchical'])\n",
    "\n",
    "print(f\"\\nMétricas del modelo jerárquico:\")\n",
    "print(f\"  Silhouette Score: {hierarchical_silhouette:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {hierarchical_db:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {hierarchical_ch:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de clusters jerárquicos en PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle(f'Visualización de Clustering Jerárquico (k={n_clusters_hierarchical}) en Espacio PCA', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_Hierarchical'], \n",
    "                           cmap='plasma', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "axes[0].set_title('Clusters Jerárquicos en PC1 vs PC2', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "scatter2 = axes[1].scatter(X_pca[:, 1], X_pca[:, 2], \n",
    "                           c=df_clustering['Cluster_Hierarchical'], \n",
    "                           cmap='plasma', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Componente Principal 2', fontsize=11)\n",
    "axes[1].set_ylabel('Componente Principal 3', fontsize=11)\n",
    "axes[1].set_title('Clusters Jerárquicos en PC2 vs PC3', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 10: DBSCAN - Density-Based Clustering\n",
    "\n",
    "Aplicación de DBSCAN para identificar clusters de densidad variable y outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "print(\"=\" * 80)\n",
    "print(\"DBSCAN - DENSITY-BASED SPATIAL CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Probar diferentes valores de epsilon\n",
    "print(\"\\nBuscando parámetros óptimos de DBSCAN...\")\n",
    "\n",
    "dbscan = DBSCAN(eps=2.0, min_samples=10)\n",
    "df_clustering['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(df_clustering['Cluster_DBSCAN'])) - (1 if -1 in df_clustering['Cluster_DBSCAN'] else 0)\n",
    "n_noise = list(df_clustering['Cluster_DBSCAN']).count(-1)\n",
    "\n",
    "print(f\"\\n✓ DBSCAN aplicado con eps=2.0, min_samples=10\")\n",
    "print(f\"\\nResultados:\")\n",
    "print(f\"  Número de clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"  Puntos clasificados como ruido (outliers): {n_noise} ({n_noise/len(df_clustering)*100:.1f}%)\")\n",
    "\n",
    "if n_clusters_dbscan > 0:\n",
    "    print(f\"\\nDistribución de clusters DBSCAN:\")\n",
    "    dbscan_distribution = df_clustering[df_clustering['Cluster_DBSCAN'] != -1]['Cluster_DBSCAN'].value_counts().sort_index()\n",
    "    for cluster_id, count in dbscan_distribution.items():\n",
    "        percentage = (count / len(df_clustering) * 100)\n",
    "        print(f\"  Cluster {cluster_id}: {count} observaciones ({percentage:.1f}%)\")\n",
    "\n",
    "    # Métricas (excluyendo ruido)\n",
    "    df_dbscan_no_noise = df_clustering[df_clustering['Cluster_DBSCAN'] != -1]\n",
    "    X_dbscan_no_noise = X_scaled[df_clustering['Cluster_DBSCAN'] != -1]\n",
    "    \n",
    "    if len(df_dbscan_no_noise) > 0 and n_clusters_dbscan > 1:\n",
    "        dbscan_silhouette = silhouette_score(X_dbscan_no_noise, df_dbscan_no_noise['Cluster_DBSCAN'])\n",
    "        dbscan_db = davies_bouldin_score(X_dbscan_no_noise, df_dbscan_no_noise['Cluster_DBSCAN'])\n",
    "        dbscan_ch = calinski_harabasz_score(X_dbscan_no_noise, df_dbscan_no_noise['Cluster_DBSCAN'])\n",
    "        \n",
    "        print(f\"\\nMétricas del modelo DBSCAN (sin ruido):\")\n",
    "        print(f\"  Silhouette Score: {dbscan_silhouette:.4f}\")\n",
    "        print(f\"  Davies-Bouldin Index: {dbscan_db:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz Index: {dbscan_ch:.2f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ DBSCAN no encontró clusters significativos con estos parámetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización DBSCAN\n",
    "if n_clusters_dbscan > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    fig.suptitle(f'Visualización de DBSCAN ({n_clusters_dbscan} clusters, {n_noise} outliers) en Espacio PCA', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Crear colormap personalizado con color especial para outliers\n",
    "    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                               c=df_clustering['Cluster_DBSCAN'], \n",
    "                               cmap='tab10', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    axes[0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "    axes[0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "    axes[0].set_title('DBSCAN en PC1 vs PC2 (ruido en -1)', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    \n",
    "    scatter2 = axes[1].scatter(X_pca[:, 1], X_pca[:, 2], \n",
    "                               c=df_clustering['Cluster_DBSCAN'], \n",
    "                               cmap='tab10', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    axes[1].set_xlabel('Componente Principal 2', fontsize=11)\n",
    "    axes[1].set_ylabel('Componente Principal 3', fontsize=11)\n",
    "    axes[1].set_title('DBSCAN en PC2 vs PC3 (ruido en -1)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 11: Comparación de Métodos de Clustering\n",
    "\n",
    "Análisis comparativo de los tres algoritmos implementados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa de métricas\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACIÓN DE MÉTODOS DE CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = {\n",
    "    'Método': ['K-Means', 'Jerárquico', 'DBSCAN'],\n",
    "    'N° Clusters': [\n",
    "        optimal_k,\n",
    "        n_clusters_hierarchical,\n",
    "        n_clusters_dbscan\n",
    "    ],\n",
    "    'Silhouette Score': [\n",
    "        final_silhouette,\n",
    "        hierarchical_silhouette,\n",
    "        dbscan_silhouette if n_clusters_dbscan > 1 else np.nan\n",
    "    ],\n",
    "    'Davies-Bouldin Index': [\n",
    "        final_db,\n",
    "        hierarchical_db,\n",
    "        dbscan_db if n_clusters_dbscan > 1 else np.nan\n",
    "    ],\n",
    "    'Calinski-Harabasz Index': [\n",
    "        final_ch,\n",
    "        hierarchical_ch,\n",
    "        dbscan_ch if n_clusters_dbscan > 1 else np.nan\n",
    "    ],\n",
    "    'Outliers Detectados': [\n",
    "        0,\n",
    "        0,\n",
    "        n_noise\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nInterpretación de métricas:\")\n",
    "print(\"  • Silhouette Score: Más alto es mejor (rango: -1 a 1)\")\n",
    "print(\"  • Davies-Bouldin Index: Más bajo es mejor (rango: 0 a ∞)\")\n",
    "print(\"  • Calinski-Harabasz Index: Más alto es mejor (rango: 0 a ∞)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('Comparación Visual de Métodos de Clustering en PC1 vs PC2', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_KMeans'], \n",
    "                           cmap='viridis', s=40, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "axes[0].set_title(f'K-Means (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# Jerárquico\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_Hierarchical'], \n",
    "                           cmap='plasma', s=40, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "axes[1].set_title(f'Jerárquico (k={n_clusters_hierarchical})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "# DBSCAN\n",
    "scatter3 = axes[2].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=df_clustering['Cluster_DBSCAN'], \n",
    "                           cmap='tab10', s=40, alpha=0.6, edgecolors='black', linewidth=0.3)\n",
    "axes[2].set_title(f'DBSCAN ({n_clusters_dbscan} clusters, {n_noise} outliers)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('PC1')\n",
    "axes[2].set_ylabel('PC2')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 12: Interpretación de Negocio - Insights Accionables\n",
    "\n",
    "Traducción de resultados técnicos a recomendaciones estratégicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de palabras clave por cluster (K-Means)\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE PALABRAS CLAVE POR CLUSTER (K-MEANS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(df_clustering['Cluster_KMeans'].unique()):\n",
    "    cluster_comments = df_clustering[df_clustering['Cluster_KMeans'] == cluster_id]['Comentario']\n",
    "    \n",
    "    # Unir todos los comentarios\n",
    "    all_text = ' '.join(cluster_comments.astype(str))\n",
    "    \n",
    "    # Tokenización simple\n",
    "    words = all_text.lower().split()\n",
    "    \n",
    "    # Filtrar palabras comunes (stopwords básico)\n",
    "    stopwords = ['el', 'la', 'de', 'y', 'a', 'en', 'es', 'por', 'con', 'los', 'las',\n",
    "                 'del', 'un', 'una', 'para', 'al', 'que', 'se', 'su', 'me', 'mi',\n",
    "                 'muy', 'pero', 'más', 'son', 'como', 'lo', 'le', 'ha']\n",
    "    words_filtered = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    \n",
    "    # Contar frecuencias\n",
    "    word_freq = Counter(words_filtered)\n",
    "    top_words = word_freq.most_common(10)\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for word, freq in top_words:\n",
    "        print(f\"  {word}: {freq} menciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard de insights por cluster\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN EJECUTIVO - INSIGHTS POR CLUSTER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(df_clustering['Cluster_KMeans'].unique()):\n",
    "    cluster_data = df_clustering[df_clustering['Cluster_KMeans'] == cluster_id]\n",
    "    \n",
    "    # Calcular métricas clave\n",
    "    avg_rating = cluster_data['Rating'].mean()\n",
    "    size = len(cluster_data)\n",
    "    size_pct = size / len(df_clustering) * 100\n",
    "    sentiment_mode = cluster_data['Sentimiento_Derivado'].mode()[0]\n",
    "    top_product = cluster_data['Producto'].mode()[0]\n",
    "    top_channel = cluster_data['Canal_Feedback'].mode()[0]\n",
    "    new_clients_pct = (cluster_data['Cliente_Tipo'] == 'Nuevo').sum() / len(cluster_data) * 100\n",
    "    \n",
    "    print(f\"\\n╔{'═' * 78}╗\")\n",
    "    print(f\"║ CLUSTER {cluster_id}: {sentiment_mode.upper()} - {size} clientes ({size_pct:.1f}%)\" + \" \" * (78 - len(f\" CLUSTER {cluster_id}: {sentiment_mode.upper()} - {size} clientes ({size_pct:.1f}%)\") - 1) + \"║\")\n",
    "    print(f\"╠{'═' * 78}╣\")\n",
    "    \n",
    "    # Perfil\n",
    "    print(f\"║ 📊 Rating Promedio: {avg_rating:.2f}/5.0\" + \" \" * (78 - len(f\" 📊 Rating Promedio: {avg_rating:.2f}/5.0\") - 1) + \"║\")\n",
    "    print(f\"║ 🎯 Producto Principal: {top_product}\" + \" \" * (78 - len(f\" 🎯 Producto Principal: {top_product}\") - 1) + \"║\")\n",
    "    print(f\"║ 📱 Canal Predominante: {top_channel}\" + \" \" * (78 - len(f\" 📱 Canal Predominante: {top_channel}\") - 1) + \"║\")\n",
    "    print(f\"║ 🆕 Clientes Nuevos: {new_clients_pct:.1f}%\" + \" \" * (78 - len(f\" 🆕 Clientes Nuevos: {new_clients_pct:.1f}%\") - 1) + \"║\")\n",
    "    \n",
    "    # Caracterización\n",
    "    if avg_rating >= 4.0:\n",
    "        caracterizacion = \"Clientes SATISFECHOS - Prioridad: RETENCIÓN\"\n",
    "    elif avg_rating >= 3.0:\n",
    "        caracterizacion = \"Clientes NEUTRALES - Prioridad: MEJORA CONTINUA\"\n",
    "    else:\n",
    "        caracterizacion = \"Clientes EN RIESGO - Prioridad: INTERVENCIÓN INMEDIATA\"\n",
    "    \n",
    "    print(f\"║ ⚠️ {caracterizacion}\" + \" \" * (78 - len(f\" ⚠️ {caracterizacion}\") - 1) + \"║\")\n",
    "    print(f\"╚{'═' * 78}╝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 13: Exportación de Resultados\n",
    "\n",
    "Generación de datasets con asignaciones de clusters para análisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar dataset completo con clusters asignados\n",
    "output_columns = [\n",
    "    'ID_Comentario', 'Fecha', 'Producto', 'Canal_Feedback', 'Comentario',\n",
    "    'Rating', 'Cliente_Tipo', 'Región', \n",
    "    'Cluster_KMeans', 'Cluster_Hierarchical', 'Cluster_DBSCAN',\n",
    "    'Sentimiento_Derivado', 'Longitud_Comentario', 'Num_Palabras'\n",
    "]\n",
    "\n",
    "df_export = df_clustering[output_columns].copy()\n",
    "\n",
    "# Guardar a CSV\n",
    "output_filename = 'feedback_con_clusters.csv'\n",
    "df_export.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTACIÓN DE RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Dataset exportado exitosamente: {output_filename}\")\n",
    "print(f\"  Dimensiones: {df_export.shape[0]} filas x {df_export.shape[1]} columnas\")\n",
    "print(f\"\\nColumnas exportadas:\")\n",
    "for col in output_columns:\n",
    "    print(f\"  • {col}\")\n",
    "\n",
    "# Resumen estadístico del export\n",
    "print(\"\\nPrimeras 10 filas del dataset exportado:\")\n",
    "display(df_export.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BLOQUE 14: Conclusiones y Recomendaciones Estratégicas\n",
    "\n",
    "Síntesis ejecutiva para stakeholders y próximos pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES ESTRATÉGICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════════════════╗\n",
    "║                           RESUMEN EJECUTIVO                                    ║\n",
    "╠════════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                                ║\n",
    "║  1. HALLAZGOS PRINCIPALES                                                     ║\n",
    "║     • Se identificaron {} clusters significativos de clientes                 ║\n",
    "║     • K-Means demostró ser el método más balanceado (Silhouette: {:.3f})      ║\n",
    "║     • Se detectaron {} outliers mediante DBSCAN                               ║\n",
    "║                                                                                ║\n",
    "║  2. SEGMENTOS CLAVE IDENTIFICADOS                                             ║\n",
    "║     • Analizar cada cluster para entender patrones de satisfacción           ║\n",
    "║     • Priorizar acciones en clusters con bajo rating                          ║\n",
    "║     • Replicar prácticas exitosas de clusters con alto rating                ║\n",
    "║                                                                                ║\n",
    "║  3. RECOMENDACIONES TÁCTICAS                                                  ║\n",
    "║     ✓ Intervención inmediata en clusters con Rating < 3.0                     ║\n",
    "║     ✓ Programas de fidelización para clusters de alto valor                  ║\n",
    "║     ✓ Optimización de canales según preferencias por cluster                 ║\n",
    "║     ✓ Personalización de productos por segmento                               ║\n",
    "║                                                                                ║\n",
    "║  4. PRÓXIMOS PASOS                                                            ║\n",
    "║     → Análisis de sentimiento textual con NLP avanzado                        ║\n",
    "║     → Modelos predictivos de churn por cluster                                ║\n",
    "║     → Cálculo de Customer Lifetime Value (CLV) segmentado                     ║\n",
    "║     → Dashboards interactivos para monitoreo continuo                         ║\n",
    "║                                                                                ║\n",
    "╚════════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\".format(\n",
    "    optimal_k,\n",
    "    final_silhouette,\n",
    "    n_noise\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EJERCICIO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Todos los bloques ejecutados\")\n",
    "print(\"✓ Modelos de clustering entrenados y evaluados\")\n",
    "print(\"✓ Insights de negocio generados\")\n",
    "print(\"✓ Resultados exportados\")\n",
    "print(\"\\nEl análisis está listo para ser presentado a stakeholders.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
